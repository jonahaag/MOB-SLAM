{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Model-based SLAM MobSLAM is a Simultaneous Localization and Mapping system for deformable environments. This repository contains a graphical user interface that enables the display of a SOFA simulation different options to record and select different paths as well as a keyboard controller for the simulation camera different options to deform the simulated object use of a Simultaneous Localization and Mapping (SLAM) algorithm to track the camera movement within the simulation generate a map of the scene incorporate deformation information based on a parallel simulation It is designed to be modular and easily extensible and aims at providing a reliable and replicable code base for future work on this topic. The model-based approach enables the SLAM algorithm to produce good results even in the presence of deformations e.g. caused by external forces or changes in volume. The aim of this work is to provide a more reliable geometric representation of deformable structures based on a monocular camera by directly incorporating information about the deformation into the mapping. The basic idea is to run a parallel FEM simulation predicting the deformation based on measurements and then use the simulated surface information, i.e. vertex positions, to continuously update the SLAM results. At this stage there is no distinction between the \"real world\" and the simulation which basically corresponds to perfect predictions of the deformation. This is obviously almost impossible to achieve in real life which is why the interface is designed to easily separate between the two views - more information on this can be found below. This repository was created as part of the research area B1 concerning Intraoperative Navigation of Multimodal sensors of the RTG 2543: Intraoperative Multisensory Tissue Differentiation in Oncology at the Institute for System Dynamics (ISYS) , University of Stuttgart, Germany. It builds on top of QSofaGLViewTools and MATLAB Monocular Visual Simultaneous Localization and Mapping . Repository overview The source code as well as all relevant information can be found in the src folder. It is split up into: the main script main.py to run the GUI the subfolder gui containing mainwindow.py : contains all the GUI elements (Widgets, Layouts, interactive elements,...) as well as the SOFA viewer and an instance of EngineORB orb.py : contains the class that is responsible for transformation and exchange of data between the SLAM (MATLAB) and the SOFA simulation (Python) plotter.py : contains functions to plot the results using pyqtgraph the subfolder mesh containing some basic mesh files and texture images that were used for testing the subfolder slam containing the MATLAB implementation of ORB-SLAM the added functions and scripts for projection and forward prediction based on the simulated deformation, i.e. the core part of the model-based SLAM the subfolder sofaviewer containing a past version of psomers3/QSofaGLViewTools : > A small PyQt widget library for viewing SOFA simulation cameras and controlling them. the subfolder feature_graph which uses opencv-python to extract ORB features of the current image creates a NetworkX graph with the features as nodes and user-setting-based edges displays that graph on top of the image Please refer to the homepage for more information on contents. There is also a small introduction available to get going. Prerequisites Python, implemented in 3.7, not tested in other versions MATLAB , implemented in 2020b, not tested with older versions Sofa with Python3 bindings qtpy , PyQt5 pyqtgraph NetworkX cv2 qdarkstyle Open issues Please see the issues section of the GitHub repository for existing issues and bugs that have not be fully resolved yet. A major bottleneck of the RSE infrastructure is the lack of proper testing, poor error handling, especially between MATLAB and Python, and missing automation regarding CI/GitHub workflows. Furthermore, checking data types of user inputs, e.g. when using the settings dialog, is an obvious area for potential improvement.","title":"Home"},{"location":"index.html#model-based-slam","text":"MobSLAM is a Simultaneous Localization and Mapping system for deformable environments. This repository contains a graphical user interface that enables the display of a SOFA simulation different options to record and select different paths as well as a keyboard controller for the simulation camera different options to deform the simulated object use of a Simultaneous Localization and Mapping (SLAM) algorithm to track the camera movement within the simulation generate a map of the scene incorporate deformation information based on a parallel simulation It is designed to be modular and easily extensible and aims at providing a reliable and replicable code base for future work on this topic. The model-based approach enables the SLAM algorithm to produce good results even in the presence of deformations e.g. caused by external forces or changes in volume. The aim of this work is to provide a more reliable geometric representation of deformable structures based on a monocular camera by directly incorporating information about the deformation into the mapping. The basic idea is to run a parallel FEM simulation predicting the deformation based on measurements and then use the simulated surface information, i.e. vertex positions, to continuously update the SLAM results. At this stage there is no distinction between the \"real world\" and the simulation which basically corresponds to perfect predictions of the deformation. This is obviously almost impossible to achieve in real life which is why the interface is designed to easily separate between the two views - more information on this can be found below. This repository was created as part of the research area B1 concerning Intraoperative Navigation of Multimodal sensors of the RTG 2543: Intraoperative Multisensory Tissue Differentiation in Oncology at the Institute for System Dynamics (ISYS) , University of Stuttgart, Germany. It builds on top of QSofaGLViewTools and MATLAB Monocular Visual Simultaneous Localization and Mapping .","title":"Model-based SLAM"},{"location":"index.html#repository-overview","text":"The source code as well as all relevant information can be found in the src folder. It is split up into: the main script main.py to run the GUI the subfolder gui containing mainwindow.py : contains all the GUI elements (Widgets, Layouts, interactive elements,...) as well as the SOFA viewer and an instance of EngineORB orb.py : contains the class that is responsible for transformation and exchange of data between the SLAM (MATLAB) and the SOFA simulation (Python) plotter.py : contains functions to plot the results using pyqtgraph the subfolder mesh containing some basic mesh files and texture images that were used for testing the subfolder slam containing the MATLAB implementation of ORB-SLAM the added functions and scripts for projection and forward prediction based on the simulated deformation, i.e. the core part of the model-based SLAM the subfolder sofaviewer containing a past version of psomers3/QSofaGLViewTools : > A small PyQt widget library for viewing SOFA simulation cameras and controlling them. the subfolder feature_graph which uses opencv-python to extract ORB features of the current image creates a NetworkX graph with the features as nodes and user-setting-based edges displays that graph on top of the image Please refer to the homepage for more information on contents. There is also a small introduction available to get going.","title":"Repository overview"},{"location":"index.html#prerequisites","text":"Python, implemented in 3.7, not tested in other versions MATLAB , implemented in 2020b, not tested with older versions Sofa with Python3 bindings qtpy , PyQt5 pyqtgraph NetworkX cv2 qdarkstyle","title":"Prerequisites"},{"location":"index.html#open-issues","text":"Please see the issues section of the GitHub repository for existing issues and bugs that have not be fully resolved yet. A major bottleneck of the RSE infrastructure is the lack of proper testing, poor error handling, especially between MATLAB and Python, and missing automation regarding CI/GitHub workflows. Furthermore, checking data types of user inputs, e.g. when using the settings dialog, is an obvious area for potential improvement.","title":"Open issues"},{"location":"feature_graph.html","text":"Feature Graph Feature matching basics A big challenge of operating in deformable environments is the compromising of feature matching, which is a important part of the standard SLAM processing. Feature matching refers to the act of finding the same features across a series of images such that in later steps the scene can be reconstructed based on the overlap between multiple images using triangulation. Typically, performance as well as precision of the entire algorithm relies heavily on the accuracy of the matching. Problems in deformable enviroments In the presence of deformations, finding suitable matches betwenn the set of detected features usually is hard due to two main reasons: 1. Changes in the relative position of the features. This might not be too big of an issue for the matching itself, but definitely can lead to errors when trying to find the proper alignment of the images based on those matches. 1. Visual appearance of the features might change between recordings. Concept of graph-based feature matching One way to deal with this is to use a graph-based approaches. The main idea here is to create a graph using the features as vertices and defining the edges e.g. based on some visual markers. This allows for the additional definition of constraints regardings the edge and vertex positions and connections and can therefore increse robustness and performance of the feature matching in challenging scenarios. Contents However, implementing this completely was out of scope for this project, which is why only the basics of creating a feature graph are considered here. For this, ORB features are detected in the current image and used as vertices for the graph. Then, each vertex is connected to all other vertices that lie within a certain distance of that vertex and the final graph is drawn on top of the image and displayed in the GUI. User settings regarding the maximum distance for two vertices to be connected, the maximum number of features, and the frequency of the graph extraction exist.","title":"Feature Graph"},{"location":"feature_graph.html#feature-graph","text":"","title":"Feature Graph"},{"location":"feature_graph.html#feature-matching-basics","text":"A big challenge of operating in deformable environments is the compromising of feature matching, which is a important part of the standard SLAM processing. Feature matching refers to the act of finding the same features across a series of images such that in later steps the scene can be reconstructed based on the overlap between multiple images using triangulation. Typically, performance as well as precision of the entire algorithm relies heavily on the accuracy of the matching.","title":"Feature matching basics"},{"location":"feature_graph.html#problems-in-deformable-enviroments","text":"In the presence of deformations, finding suitable matches betwenn the set of detected features usually is hard due to two main reasons: 1. Changes in the relative position of the features. This might not be too big of an issue for the matching itself, but definitely can lead to errors when trying to find the proper alignment of the images based on those matches. 1. Visual appearance of the features might change between recordings.","title":"Problems in deformable enviroments"},{"location":"feature_graph.html#concept-of-graph-based-feature-matching","text":"One way to deal with this is to use a graph-based approaches. The main idea here is to create a graph using the features as vertices and defining the edges e.g. based on some visual markers. This allows for the additional definition of constraints regardings the edge and vertex positions and connections and can therefore increse robustness and performance of the feature matching in challenging scenarios.","title":"Concept of graph-based feature matching"},{"location":"feature_graph.html#contents","text":"However, implementing this completely was out of scope for this project, which is why only the basics of creating a feature graph are considered here. For this, ORB features are detected in the current image and used as vertices for the graph. Then, each vertex is connected to all other vertices that lie within a certain distance of that vertex and the final graph is drawn on top of the image and displayed in the GUI. User settings regarding the maximum distance for two vertices to be connected, the maximum number of features, and the frequency of the graph extraction exist.","title":"Contents"},{"location":"get_going.html","text":"Get going To open the GUI, step into the source folder and simply run main.py , e.g. via # pwd==.../Model_based_SLAM cd src python3 main.py or execute the shell script with # pwd==.../Model_based_SLAM ./run.sh First a popup dialog will open, allowing you to chose the mode consisting of the GUI layout, SLAM version, and navigation mode. The two options for the GUI layout are: Main : the intended final layout consisting of option bar (very top) real world view (top left) simulated view (bottom left) SLAM results plot (top right) feature graph window (bottom right) In this mode all the widgets are fixed in space and all the relevant information is visible at all times. Test : a layout that allow for more user configuration consisting of option bar (very top) real world view (left) SLAM results plot (right, currently in the front) feature graph window (right, in the back) This mode was designed specifically for the considered case, that is real world and simulated environment are basically the same. Here the two plots on the right are (un)dockable and can be resized and reorganized to some degree. The two options for the SLAM version refer to the algorithm that is used for the scene reconstruction and position estimation: ORB : the standard ORB-SLAM implemented in MATLAB MOB : the Model-based SLAM to keep track of deformations Note: Keep in mind that the Model-based SLAM, at least at its current stage, is quite slow compared to ORB-SLAM and does not guarantee good results in the presence of large deformations. Click Ok and the GUI will open. For details on the different elements visible and their functionalities, please refer to gui.md . Happy mapping!","title":"Get going"},{"location":"get_going.html#get-going","text":"To open the GUI, step into the source folder and simply run main.py , e.g. via # pwd==.../Model_based_SLAM cd src python3 main.py or execute the shell script with # pwd==.../Model_based_SLAM ./run.sh First a popup dialog will open, allowing you to chose the mode consisting of the GUI layout, SLAM version, and navigation mode. The two options for the GUI layout are: Main : the intended final layout consisting of option bar (very top) real world view (top left) simulated view (bottom left) SLAM results plot (top right) feature graph window (bottom right) In this mode all the widgets are fixed in space and all the relevant information is visible at all times. Test : a layout that allow for more user configuration consisting of option bar (very top) real world view (left) SLAM results plot (right, currently in the front) feature graph window (right, in the back) This mode was designed specifically for the considered case, that is real world and simulated environment are basically the same. Here the two plots on the right are (un)dockable and can be resized and reorganized to some degree. The two options for the SLAM version refer to the algorithm that is used for the scene reconstruction and position estimation: ORB : the standard ORB-SLAM implemented in MATLAB MOB : the Model-based SLAM to keep track of deformations Note: Keep in mind that the Model-based SLAM, at least at its current stage, is quite slow compared to ORB-SLAM and does not guarantee good results in the presence of large deformations. Click Ok and the GUI will open. For details on the different elements visible and their functionalities, please refer to gui.md . Happy mapping!","title":"Get going"},{"location":"gui.html","text":"GUI Basic GUI layout There exist two basic layout modes: Main (left) and Test (right) that, for the most part, consist of the same elements. The only real difference is that Test doesn't distinguish between the real world view and the simulation view (at least in terms of visualization) and therefore shows the environment only once. Option Bar The options bar consists of the following elements: start/stop button for simulation start/stop button for SLAM settings button to open settings dialog (see below) deformation slider to deform the sofa object graph extraction checkbox to switch on/turn off the graph extraction as described in feature_graph.md colortheme checkbox to switch between light and dark mode text editor to which the console output (e.g. print statements) is redirected Settings dialog Following values can be set by the user: abc","title":"GUI"},{"location":"gui.html#gui","text":"","title":"GUI"},{"location":"gui.html#basic-gui-layout","text":"There exist two basic layout modes: Main (left) and Test (right) that, for the most part, consist of the same elements. The only real difference is that Test doesn't distinguish between the real world view and the simulation view (at least in terms of visualization) and therefore shows the environment only once.","title":"Basic GUI layout"},{"location":"gui.html#option-bar","text":"The options bar consists of the following elements: start/stop button for simulation start/stop button for SLAM settings button to open settings dialog (see below) deformation slider to deform the sofa object graph extraction checkbox to switch on/turn off the graph extraction as described in feature_graph.md colortheme checkbox to switch between light and dark mode text editor to which the console output (e.g. print statements) is redirected","title":"Option Bar"},{"location":"gui.html#settings-dialog","text":"Following values can be set by the user: abc","title":"Settings dialog"},{"location":"slam.html","text":"SLAM SLAM basics Simultaneous Localization and Mapping (SLAM) is a term describing a more or less broad class of algorithms that aim at reconstructing the surrounding (mapping) and providing precise position information (localization) of a mobile agent simultaneously using the agent's sensory input data. This project considers the probably most-studied case of a monocular camera, i.e. a single camera image is supplied to the algorithm continuously. However, in the literature many different sensor configurations can be found. For the classic monocular case in a rigid enviroment, many different solution schemes have been developed and studied extensively. One of the most well-known of these is ORB-SLAM, an algorithm that relies on ORB feature detection and has been proven to deliver very good results using common public datasets and real-world data. ORB-SLAM extension for deformable enviroments In this project deformable environments are considered, which can cause the estimated and actual camera position to diverge and also leads to inaccurate mapping of the environment when using the \"standard\" (unmodified) ORB-SLAM. To act upon these emerging diffuculties, this proof-of-concept tries to use an existing environment model to predict the ongoing deformation. The predictions are then used to update the results of the standard algorithm to still produce satisfactory results. MATLAB implementation The src/slam folder contains a MATLAB implementation of ORB-SLAM that is available via MathWorks . While the predicted deformation is computed in Python using SOFA, incorporating that information into the ongoing SLAM process is done in MATLAB. The main contributions are located in the projectionAndPrediction folder . The basic concept of the model-based approach contains methods and scripts to perform the following steps: projecting new map points on the surface of the environment model projectMapPoints.m cameraViewProjection.m updating the position of points that were projected at previous iterations based on new predictions of the deformation (forward prediction) forwardPrediction.m updating the correspondend 2D position of the feature in the image updateFeaturePosition.m keeping track of projected points, removing points that are no longer considered addProjectedPoints.m removeProjectedPoints.m removeUnprojectedPoints.m The remaining functions are used as helper functions or are deprecated but kept for testing purposes.","title":"SLAM"},{"location":"slam.html#slam","text":"","title":"SLAM"},{"location":"slam.html#slam-basics","text":"Simultaneous Localization and Mapping (SLAM) is a term describing a more or less broad class of algorithms that aim at reconstructing the surrounding (mapping) and providing precise position information (localization) of a mobile agent simultaneously using the agent's sensory input data. This project considers the probably most-studied case of a monocular camera, i.e. a single camera image is supplied to the algorithm continuously. However, in the literature many different sensor configurations can be found. For the classic monocular case in a rigid enviroment, many different solution schemes have been developed and studied extensively. One of the most well-known of these is ORB-SLAM, an algorithm that relies on ORB feature detection and has been proven to deliver very good results using common public datasets and real-world data.","title":"SLAM basics"},{"location":"slam.html#orb-slam-extension-for-deformable-enviroments","text":"In this project deformable environments are considered, which can cause the estimated and actual camera position to diverge and also leads to inaccurate mapping of the environment when using the \"standard\" (unmodified) ORB-SLAM. To act upon these emerging diffuculties, this proof-of-concept tries to use an existing environment model to predict the ongoing deformation. The predictions are then used to update the results of the standard algorithm to still produce satisfactory results.","title":"ORB-SLAM extension for deformable enviroments"},{"location":"slam.html#matlab-implementation","text":"The src/slam folder contains a MATLAB implementation of ORB-SLAM that is available via MathWorks . While the predicted deformation is computed in Python using SOFA, incorporating that information into the ongoing SLAM process is done in MATLAB. The main contributions are located in the projectionAndPrediction folder . The basic concept of the model-based approach contains methods and scripts to perform the following steps: projecting new map points on the surface of the environment model projectMapPoints.m cameraViewProjection.m updating the position of points that were projected at previous iterations based on new predictions of the deformation (forward prediction) forwardPrediction.m updating the correspondend 2D position of the feature in the image updateFeaturePosition.m keeping track of projected points, removing points that are no longer considered addProjectedPoints.m removeProjectedPoints.m removeUnprojectedPoints.m The remaining functions are used as helper functions or are deprecated but kept for testing purposes.","title":"MATLAB implementation"},{"location":"sofa_viewer.html","text":"Sofa Viewer Based on a past version of psomers3/QSofaGLViewTools this is a small PyQt widget library for viewing SOFA simulation cameras and controlling them. All of the used components are described in gui.md and src/gui/mainwindow.py in greater detail.","title":"Sofa Viewer"},{"location":"sofa_viewer.html#sofa-viewer","text":"Based on a past version of psomers3/QSofaGLViewTools this is a small PyQt widget library for viewing SOFA simulation cameras and controlling them. All of the used components are described in gui.md and src/gui/mainwindow.py in greater detail.","title":"Sofa Viewer"}]}